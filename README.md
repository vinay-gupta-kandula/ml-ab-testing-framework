# ğŸš€ ML A/B Testing Framework

This project implements a **production-style A/B testing system** for machine learning models.  
It demonstrates how real-world companies test ML systems by routing live traffic to multiple models and collecting evidence to decide which is better.

---

## ğŸ¯ What This Project Includes

* ğŸ§  Two trained ML models (Random Forest & Logistic Regression)
* âš¡ FastAPI prediction API with A/B split (50/50 traffic)
* ğŸ—„ï¸ SQLite logging of predictions + latency
* ğŸ“Š Offline statistical analysis (t-tests)
* ğŸ“º Streamlit dashboard for experiment monitoring
* ğŸ“¦ Docker containerization for reproducible deployment
* ğŸ§ª PyTest suite validating API behavior

---

## ğŸ“ Project Structure

```

ml-ab-testing-framework/
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py               # FastAPI service + logging + A/B routing
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ model_A.pkl       # Logistic Regression
â”‚   â”‚   â”œâ”€â”€ model_B.pkl       # Random Forest
â”‚   â”‚   â””â”€â”€ feature_cols.pkl  # 46 feature columns
â”‚   â””â”€â”€ **init**.py
â”‚
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ run_analysis.py       # Statistical evaluation + JSON export
â”‚   â”œâ”€â”€ dashboard.py          # Streamlit UI for visual analytics
â”‚   â””â”€â”€ **init**.py
â”‚
â”œâ”€â”€ data/                     # Holds SQLite DB (persisted on disk)
â”‚   â””â”€â”€ telco/
â”‚       â””â”€â”€ WA_Fn-UseC_-Telco-Customer-Churn.csv
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_api.py           # Validates API + logging
â”‚   â””â”€â”€ **init**.py
â”‚
â”œâ”€â”€ train_models.py           # Trains both models from dataset
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ README.md
â”œâ”€â”€ METHODOLOGY.md
â”œâ”€â”€ video.txt                 # contains public url to access the video 
â””â”€â”€ submission.yml


````

---

# ğŸ§  STEP 1 â€” Train Models (Run Once)

```bash
python train_models.py
````

This loads the Telco dataset, generates features, and saves:

* `model_A.pkl`
* `model_B.pkl`
* `feature_cols.pkl`

---

# ğŸš¢ STEP 2 â€” Build & Run with Docker

### Build

```bash
docker-compose build
```

### Run container (API + DB)

```bash
docker-compose up -d
```

### Verify running

```bash
docker ps
```

---

# ğŸ STEP 3 â€” Make Predictions Through API

Open:

```
http://localhost:8000/docs
```

Select **POST /predict**
Paste EXACTLY this JSON (46 features):

```json
{
  "features": [0,1,29.85,29.85,1,0,0,1,1,0,1,0,0,1,0,1,0,0,1,0,0,0,0,1,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,0,1,0,0,1,0,1]
}
```

Each request is randomly routed:

* Model A (Logistic Regression)
* Model B (Random Forest)

Requests are logged in `/data/ab_test_logs.db`.

---

# ğŸ“Š STEP 4 â€” Run A/B Test Statistical Analysis

After sending several requests:

```bash
docker-compose exec api python analysis/run_analysis.py
```

This prints:

* Count of A vs B samples
* Mean prediction comparison
* Latency t-test (p-value)

Results saved to:

```
analysis/results.json
```

View it:

```bash
docker-compose exec api cat analysis/results.json
```

---

# ğŸ“º STEP 5 â€” View A/B Dashboard (Streamlit)

Launch dashboard inside Docker:

```bash
docker-compose exec api streamlit run analysis/dashboard.py --server.address=0.0.0.0 --server.port=8501
```

Streamlit prints:

```
URL: http://0.0.0.0:8501
```

OPEN this in browser (not 0.0.0.0):

```
http://localhost:8501
```

OR

```
http://127.0.0.1:8501
```

Dashboard Shows:
âœ” Logged requests
âœ” A vs B request split
âœ” Prediction averages
âœ” p-value significance

---

# ğŸ§ª STEP 6 â€” Run Tests

```bash
docker-compose exec api pytest -q
```

Expected output:

```
..
2 passed
```

---

# ğŸ›‘ STEP 7 â€” Shut Down System

```bash
docker-compose down
```

Database remains safe in:

```
./data/ab_test_logs.db
```

---

# âš¡ Local (Non-Docker) Mode

### Install dependencies

```bash
pip install -r api/requirements.txt
```

### Run API locally

```bash
uvicorn api.main:app --reload
```

Visit:

```
http://localhost:8000/docs
```

### Run analysis

```bash
python analysis/run_analysis.py
```

### Run Streamlit

```bash
streamlit run analysis/dashboard.py
```

Open:

```
http://localhost:8501
```

---

# ğŸŒŸ Tech Stack Summary

| Component                           | Role                 |
| ----------------------------------- | -------------------- |
| FastAPI                             | Serves ML inference  |
| Logistic Regression / Random Forest | Model variants       |
| SQLite                              | Persistent logging   |
| Pandas / NumPy                      | Feature handling     |
| SciPy                               | Statistical testing  |
| Streamlit                           | Experiment dashboard |
| Docker + Compose                    | Deployment           |
| Pytest                              | Automated testing    |


