# ğŸš€ ML A/B Testing Framework

This project implements a **production-grade A/B testing system** for machine learning models, similar to what real tech companies deploy.
It supports deterministic routing, sticky user assignment, persistent logging, automated statistics, and a live dashboard.

---

## ğŸ¯ Key Features (Submission-Ready)

* ğŸ§  Two ML variants: Logistic Regression (Control) & Random Forest (Treatment)
* ğŸ”€ **Deterministic traffic split using MD5 hashing** of `user_id`
* â™»ï¸ Sticky assignment â€” same user always gets same model
* âš¡ FastAPI serving with background DB logging
* ğŸ—„ï¸ SQLite persistence mounted to disk
* ğŸ“ˆ Welchâ€™s T-test statistical comparison
* ğŸ“º Streamlit dashboard to monitor results
* ğŸ§ª Automated pytest suite already passing
* ğŸ³ Fully containerized with Docker & Compose
* ğŸ› submission.yml for automated build â†’ deploy â†’ test â†’ analyze

---

## ğŸ“ Project Structure

```
ml-ab-testing-framework/
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py                  # FastAPI service + hashing + logging
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ model_A.pkl
â”‚   â”‚   â”œâ”€â”€ model_B.pkl
â”‚   â”‚   â””â”€â”€ feature_cols.pkl
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ run_analysis.py          # Welchâ€™s T-test + JSON results
â”‚   â”œâ”€â”€ dashboard.py             # Streamlit visual dashboard
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ data/                        # SQLite DB persists here
â”‚   â””â”€â”€ telco/
â”‚       â””â”€â”€ WA_Fn-UseC_-Telco-Customer-Churn.csv
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_api.py              # Verifies model routing & DB logging
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ train_models.py              # Train + save models & feature columns
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ README.md
â”œâ”€â”€ METHODOLOGY.md
â”œâ”€â”€ submission.yml
â””â”€â”€ video.txt
```

---

## ğŸ§  STEP 1 â€” Train Models (Run Locally Once)

```bash
python train_models.py
```

Outputs:

* `model_A.pkl`
* `model_B.pkl`
* `feature_cols.pkl`

---

## ğŸš¢ STEP 2 â€” Build & Run via Docker

### Build images

```bash
docker-compose build
```

### Start stack (API + DB volume)

```bash
docker-compose up -d
```

Verify running:

```bash
docker ps
```

---

## ğŸ STEP 3 â€” Make Deterministic API Requests

Swagger docs:

```
http://localhost:8000/docs
```

### Required JSON (46 feature values + user_id):

```json
{
  "user_id": "customer_99",
  "features": [0,1,29.85,29.85,1,0,0,1,1,0,1,0,0,1,0,1,0,0,1,0,0,0,0,1,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,0,1,0,0,1,0,1]
}
```

â¡ï¸ Same `user_id` â†’ always same model
â¡ï¸ Different user_ids â†’ balanced 50/50 split

All requests log to:

```
./data/ab_test_logs.db
```

---

## ğŸ“Š STEP 4 â€” Statistical Analysis

```bash
docker-compose exec api python analysis/run_analysis.py
```

Output includes:

* Request counts by model
* Mean prediction comparison
* Welchâ€™s T-test p-value
* Winner indication

JSON results stored in:

```
analysis/results.json
```

Inspect:

```bash
docker-compose exec api cat analysis/results.json
```

---

## ğŸ“º STEP 5 â€” View Streamlit Dashboard

Run UI:

```bash
docker-compose exec api streamlit run analysis/dashboard.py --server.address=0.0.0.0 --server.port=8501
```

Open in browser:

```
http://localhost:8501
```

Dashboard shows:
âœ” Raw logs
âœ” A/B request volume
âœ” Mean predicted churn per model
âœ” p-value significance

---

## ğŸ§ª STEP 6 â€” Run Automated Tests

```bash
docker-compose exec api pytest -q
```

Expected:

```
..
2 passed
```

---

## ğŸ›‘ STEP 7 â€” Shut Down

```bash
docker-compose down
```

SQLite logs are preserved in:

```
./data/ab_test_logs.db
```

---

## âš¡ OPTIONAL â€” Run Without Docker

Install deps:

```bash
pip install -r api/requirements.txt
```

Run API:

```bash
uvicorn api.main:app --reload
```

Run analysis:

```bash
python analysis/run_analysis.py
```

Dashboard:

```bash
streamlit run analysis/dashboard.py
```

---

## ğŸŒŸ Core Tech Stack

| Component                           | Role                            |
| ----------------------------------- | ------------------------------- |
| FastAPI                             | Model serving, hashing, logging |
| Logistic Regression / Random Forest | Competing ML variants           |
| SQLite                              | Persistent A/B logging          |
| SciPy                               | Welchâ€™s T-test                  |
| Streamlit                           | Live experiment dashboard       |
| Docker + Compose                    | Runtime consistency             |
| Pytest                              | Automated verification          |


