# ðŸ“‘ A/B Testing Methodology

## ðŸŽ¯ Goal
Evaluate which ML model (A: Logistic Regression or B: Random Forest)
provides better churn predictions under real traffic distribution.

## ðŸ§ª Hypothesis
- **Null H0**: There is no difference between Model A and Model B.
- **Alternative H1**: One model performs significantly better.

## ðŸ“Š Metrics Collected
| Metric | Why |
|--------|-----|
| Prediction value | Proxy for model output |
| Request volume | Ensures even A/B traffic |
| Latency per request | Measures speed differences |

## ðŸ”€ Traffic Routing
Incoming requests are randomly split:
- Model A â†’ 50%
- Model B â†’ 50%

This simulates real-world inference load balancing.

## ðŸ“ˆ Statistical Test
We apply a **two-sample t-test (unequal variance)** on request latency:

- If **p-value < 0.05** â†’ statistically significant
- If **p-value >= 0.05** â†’ no meaningful difference

## ðŸ§  Decision Criteria
- If Model B consistently shows lower latency & better prediction behavior â†’ declare B winner
- If results inconclusive â†’ continue collecting samples

## ðŸ—„ Data Logging
Every prediction request logs:
- Model variant
- Timestamp
- Input features
- Prediction value
- Latency

Stored inside `data/ab_test_logs.db`.

## âœ” Final Notes
This framework can be extended with:
- More variants (A/B/C)
- Real ground-truth tracking
- Advanced significance tests
